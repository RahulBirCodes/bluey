{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1724f81c",
   "metadata": {},
   "source": [
    "## Bluey-MERDifold Run Jobs to test Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c6613",
   "metadata": {},
   "source": [
    "## Environment set up\n",
    "Set up pyproject.toml or uv.lock\n",
    "Don't think we need to clone the repo from Github\n",
    "Maybe mount the Google Drive and link the checkpoints-directory to be somewhere in the Google Drive\n",
    "Wandb authorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb500413",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a02bba48",
   "metadata": {},
   "source": [
    "## msign function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e9352",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ABC_LIST: list[tuple[float, float, float]] = [\n",
    "    (8.28721201814563, -23.595886519098837, 17.300387312530933),\n",
    "    (4.107059111542203, -2.9478499167379106, 0.5448431082926601),\n",
    "    (3.9486908534822946, -2.908902115962949, 0.5518191394370137),\n",
    "    (3.3184196573706015, -2.488488024314874, 0.51004894012372),\n",
    "    (2.300652019954817, -1.6689039845747493, 0.4188073119525673),\n",
    "    (1.891301407787398, -1.2679958271945868, 0.37680408948524835),\n",
    "    (1.8750014808534479, -1.2500016453999487, 0.3750001645474248),\n",
    "    (1.875, -1.25, 0.375),\n",
    "]\n",
    "\n",
    "# safety factor for numerical stability (but exclude last polynomial)\n",
    "ABC_LIST_STABLE: list[tuple[float, float, float]] = [\n",
    "    (a / 1.01, b / 1.01**3, c / 1.01**5) for (a, b, c) in ABC_LIST[:-1]\n",
    "] + [ABC_LIST[-1]]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def msign(G: torch.Tensor, steps: int = 10) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Polar Express algorithm for the matrix sign function:\n",
    "    https://arxiv.org/abs/2505.16932\n",
    "    \"\"\"\n",
    "    assert G.ndim >= 2\n",
    "    should_transpose: bool = G.size(-2) > G.size(-1)\n",
    "\n",
    "    x = G.bfloat16()\n",
    "    if should_transpose:\n",
    "        x = x.mT\n",
    "\n",
    "    x /= x.norm(dim=(-2, -1), keepdim=True) * 1.01\n",
    "    for step in range(steps):\n",
    "        a, b, c = ABC_LIST_STABLE[step] if step < len(ABC_LIST_STABLE) else ABC_LIST_STABLE[-1]\n",
    "        s = x @ x.mT\n",
    "        # goal is to compute x = a x + b S x + c S^2 x\n",
    "        # we can break this up into: x = (a I + (b I + c S) S) x\n",
    "        y = c * s\n",
    "        y.diagonal(dim1=-2, dim2=-1).add_(b)\n",
    "        y = y @ s\n",
    "        y.diagonal(dim1=-2, dim2=-1).add_(a)\n",
    "        x = y @ x\n",
    "\n",
    "    if should_transpose:\n",
    "        x = x.mT\n",
    "    x = torch.nan_to_num(x)\n",
    "    return x.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec32fa4",
   "metadata": {},
   "source": [
    "## ManifoldMuonW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a67d57",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from optimizers.msign import msign\n",
    "import math\n",
    "import torch\n",
    "\n",
    "def manifold_muon_step(\n",
    "    W: torch.Tensor,\n",
    "    G: torch.Tensor,\n",
    "    lr: float,\n",
    "    alpha: float = 0.01,\n",
    "    steps: int = 50,\n",
    "    tol: float = 1e-6,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"One manifold Muon update step keeping W on a Stiefel-like manifold.\"\"\"\n",
    "    orig_tall = True\n",
    "    if W.shape[0] < W.shape[1]:\n",
    "        # Make W tall\n",
    "        W = W.transpose(-2, -1)\n",
    "        G = G.transpose(-2, -1)\n",
    "        orig_tall = False\n",
    "\n",
    "    # Dual variable initialization\n",
    "    Lambda = -0.25 * (W.T @ G + G.T @ W)\n",
    "\n",
    "    for k in range(steps):\n",
    "        # Candidate direction in ambient space\n",
    "        A = msign(G + 2 * W @ Lambda)\n",
    "\n",
    "        # Measure tangent-space violation\n",
    "        H = W.T @ A + A.T @ W\n",
    "        if torch.norm(H) / math.sqrt(H.numel()) < tol:\n",
    "            break\n",
    "\n",
    "        # Dual ascent step with simple annealing\n",
    "        Lambda = Lambda - alpha * (1.0 - k / steps) * H\n",
    "\n",
    "    # Primal descent step\n",
    "    new_W = W - lr * A\n",
    "\n",
    "    new_W = msign(new_W)\n",
    "\n",
    "    if not orig_tall:\n",
    "        new_W = new_W.transpose(-2, -1)\n",
    "        \n",
    "    return new_W\n",
    "\n",
    "def manifold_muon_ADMM_step(\n",
    "    W: torch.Tensor,\n",
    "    G: torch.Tensor,\n",
    "    lr: float,\n",
    "    alpha: float = 0.01,\n",
    "    steps: int = 50,\n",
    "    rho: int = 4.0,\n",
    "    tol: float = 1e-6,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Implements GD on || G + W @ (L + L.mT) ||_* (c.f. the blog)\"\"\"\n",
    "    # Ensure that W and G are both tall matrices\n",
    "    should_transpose = W.shape[0] < W.shape[1]\n",
    "    if should_transpose:\n",
    "        W = W.T\n",
    "        G = G.T\n",
    "    # Initialize the lagrangian, slack, and dual variable\n",
    "    Lambda = -0.25 * (W.T @ G + G.T @ W)\n",
    "    X = G + 2 * W @ Lambda\n",
    "    Omega = torch.zeros_like(X)\n",
    "    # Solve the dual problem with ADMM to find the update direction A\n",
    "    for step in range(steps):\n",
    "        #if step % 10 == 0:\n",
    "            #print(f\"W: {W} and step: {step}\")\n",
    "        # Update for Lambda (orthonormal least-squares solve)\n",
    "        P = W.mT @ (1 / rho * Omega + X - G)\n",
    "        Lambda_upd = 0.25 * (P + P.mT)\n",
    "        # Update for X (singular value thresholding)\n",
    "        B = G + 2 * W @ Lambda_upd - 1 / rho * Omega\n",
    "        eye = torch.eye(B.shape[1], device=B.device, dtype=B.dtype)\n",
    "        P_pos = 0.5 * (eye + msign(B.mT @ B - 1 / rho**2 * eye))\n",
    "        X_upd = (B - 1 / rho * msign(B)) @ P_pos\n",
    "        # Update for Omega (dual ascent)\n",
    "        Omega_upd = Omega + rho * (X_upd - 2 * W @ Lambda_upd - G)\n",
    "        Lambda, X, Omega = Lambda_upd, X_upd, Omega_upd\n",
    "    # Calculate A from final ADMM solution\n",
    "    # (at convergence, G + 2 * W @ Lambda \\approx X)\n",
    "    A = msign(G + 2 * W @ Lambda)\n",
    "    # Descend on the primal problem\n",
    "    new_W = W - lr * A\n",
    "    # Retract to the manifold\n",
    "    new_W = msign(new_W)\n",
    "    # Restore the shape of the solution and return\n",
    "    return new_W.T if should_transpose else new_W\n",
    "\n",
    "class ManifoldMuonW(Optimizer):\n",
    "    \"\"\"\n",
    "    Hybrid optimizer:\n",
    "      - For param groups with group['manifold'] == True:\n",
    "          use manifold_muon_step (Stiefel + spectral norm) with a Muon-style\n",
    "          momentum buffer.\n",
    "      - For all other params: plain AdamW.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        betas=(0.95, 0.95),     # [0] used as Muon-style momentum; [1] for AdamW's second moment\n",
    "        weight_decay: float = 0.0,\n",
    "        eps: float = 1e-8,\n",
    "        mm_steps: int = 50,\n",
    "        mm_alpha: float = 0.01,\n",
    "        mm_tol: float = 1e-6,\n",
    "        ADMM: bool = True,\n",
    "        mm_use_momentum: bool = False,\n",
    "    ):\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon: {eps}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta1: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta2: {betas[1]}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            weight_decay=weight_decay,\n",
    "            eps=eps,\n",
    "            mm_steps=mm_steps,\n",
    "            mm_alpha=mm_alpha,\n",
    "            mm_tol=mm_tol,\n",
    "            ADMM=ADMM,\n",
    "            mm_use_momentum=mm_use_momentum,\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            #print(f\"group: {group}, group['params'] {group['params']}\")\n",
    "            lr = group[\"lr\"]\n",
    "            beta1, beta2 = group[\"betas\"]\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "            eps = group[\"eps\"]\n",
    "            mm_steps = group[\"mm_steps\"]\n",
    "            mm_alpha = group[\"mm_alpha\"]\n",
    "            mm_tol = group[\"mm_tol\"]\n",
    "            mm_use_momentum = group.get(\"mm_use_momentum\", False)\n",
    "            ADMM = group.get(\"ADMM\", True)\n",
    "            use_manifold = group.get(\"manifold\", True)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad\n",
    "\n",
    "                # Decoupled weight decay\n",
    "                if weight_decay != 0.0:\n",
    "                    p.data.mul_(1.0 - lr * weight_decay)\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # Initialize state lazily\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # AdamW stats\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "                    # Muon-style momentum for manifold params\n",
    "                    state[\"muon_m\"] = torch.zeros_like(p)\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "                exp_avg, exp_avg_sq, muon_m = (\n",
    "                    state[\"exp_avg\"],\n",
    "                    state[\"exp_avg_sq\"],\n",
    "                    state[\"muon_m\"],\n",
    "                )\n",
    "\n",
    "                # AdamW moments always maintained (even if not used)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1.0 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
    "\n",
    "                if use_manifold and p.ndim >= 2:\n",
    "                    #print(\"using manifold!\")\n",
    "                    # I think that we should just use ManifoldMuon on all of the weights\n",
    "                    # Let's not use mommentum\n",
    "                    if mm_use_momentum:\n",
    "                        muon_m.lerp_(grad, 1.0 - beta1)   # simple EMA; could tweak\n",
    "                        G_eff = muon_m\n",
    "                    else:\n",
    "                        # No momentum: use raw grad\n",
    "                        G_eff = grad\n",
    "\n",
    "                    W = p.data\n",
    "\n",
    "                    if ADMM:\n",
    "                        new_W = manifold_muon_ADMM_step(\n",
    "                            W,\n",
    "                            G_eff,\n",
    "                            lr=lr,\n",
    "                            alpha=mm_alpha,\n",
    "                            steps=mm_steps,\n",
    "                            tol=mm_tol,\n",
    "                        )\n",
    "                    else:\n",
    "                        new_W = manifold_muon_step(\n",
    "                            W,\n",
    "                            G_eff,\n",
    "                            lr=lr,\n",
    "                            alpha=mm_alpha,\n",
    "                            steps=mm_steps,\n",
    "                            tol=mm_tol,\n",
    "                        )\n",
    "                    p.data.copy_(new_W)\n",
    "\n",
    "                else:\n",
    "                    # ---- AdamW branch ----\n",
    "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
    "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
    "\n",
    "                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
    "                    step_size = lr / bias_correction1\n",
    "\n",
    "                    p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d13c",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c81a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_batch(\n",
    "    batch_size: int = 8,\n",
    "    num_pairs: int = 5,     # T\n",
    "    xy_size: int = 5,       # D\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a fresh least-squares problem every call:\n",
    "        X ~ N(0,1)           shape (B, T, D)\n",
    "        W ~ N(0,1/D)         shape (B, D, D)\n",
    "        Y = X @ W            shape (B, T, D)\n",
    "\n",
    "    Returns tokens of shape:\n",
    "        tokens: (B, 2T, 2*(D+1))\n",
    "\n",
    "    Layout per token vector:\n",
    "        [x_flag, y_flag, x_1..x_D, y_1..y_D]\n",
    "\n",
    "    Sequence layout:\n",
    "        x_1, y_1, ..., x_T,  y_T\n",
    "        or\n",
    "        y_1, x_1, ..., y_T,  x_T \n",
    "        (chosen randomly to remove any positional symmetry)\n",
    "\n",
    "    Also returns:\n",
    "        X: (B, T, D)\n",
    "        Y: (B, T, D)\n",
    "        W: (B, D, D)\n",
    "        y_pos: (B, T)\n",
    "    \"\"\"\n",
    "\n",
    "    B, T, D = batch_size, num_pairs, xy_size\n",
    "    token_dim = 2 * (D + 1)   # [x_flag, y_flag, x_D, y_D]\n",
    "    X = torch.randn(B, T, D, device=device)\n",
    "    W = torch.randn(B, D, D, device=device) / (D ** 0.5)\n",
    "    Y = torch.einsum(\"btd,bdk->btk\", X, W)\n",
    "\n",
    "    base = torch.arange(2*T, device=device)\n",
    "    swapped = base.view(T, 2).flip(1).reshape(-1)\n",
    "    flip_mask = torch.randint(0, 2, (B,), device=device)\n",
    "    pos = torch.where(\n",
    "        flip_mask[:,None] == 0,\n",
    "        base.unsqueeze(0).expand(B, 2*T),\n",
    "        swapped.unsqueeze(0).expand(B, 2*T)\n",
    "    )\n",
    "    pos_matrix = pos.view(B, T, 2)\n",
    "    x_pos = pos_matrix[:, :, 0]\n",
    "    y_pos = pos_matrix[:, :, 1]\n",
    "\n",
    "    tokens = torch.zeros(B, 2*T, token_dim, device=device)\n",
    "    b_ind = torch.arange(B, device=device).unsqueeze(1)\n",
    "    tokens[b_ind, x_pos, 0] = 1.0\n",
    "    tokens[b_ind, x_pos, 2:2+xy_size] = X\n",
    "    tokens[b_ind, y_pos, 1] = 1.0\n",
    "    tokens[b_ind, y_pos, 2+xy_size:2+2*xy_size] = Y\n",
    "    # return x_pos since model outputs y_preds there\n",
    "    return tokens, X, Y, W, x_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27ea7d",
   "metadata": {},
   "source": [
    "## sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe4b5db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from itertools import product \n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "HYPERPARAM_GRID_ADAMW = {\n",
    "    \"lr\": [3e-4, 1e-3],\n",
    "    \"beta1\": [0.9],\n",
    "    \"beta2\": [0.98],\n",
    "    \"weight_decay\": [0.0, 0.1],\n",
    "    \"batch_size\": [64, 256],\n",
    "}\n",
    "\n",
    "HYPERPARAM_GRID_MUON = {\n",
    "    \"lr\": [1e-3, 3e-3],\n",
    "    \"beta1\": [0.9],\n",
    "    \"beta2\": [0.98],\n",
    "    \"weight_decay\": [0.0, 0.05],\n",
    "    \"batch_size\": [64, 256],\n",
    "}\n",
    "\n",
    "\"\"\" \n",
    "HYPERPARAM_GRID_ADAMW = {\n",
    "    \"lr\": [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2],\n",
    "    \"beta1\": [0.85, 0.9, 0.95],\n",
    "    \"beta2\": [0.95, 0.98, 0.999],\n",
    "    \"weight_decay\": [0.0, 0.01, 0.1, 0.2],\n",
    "    \"batch_size\": [32, 64, 128, 256, 512, 1024],\n",
    "    \n",
    "} \n",
    "\n",
    "HYPERPARAM_GRID_MUON = {\n",
    "    \"lr\": [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2],\n",
    "    \"momentum\": [0.9, 0.95, 0.98],  # often you’ll just fix 0.95\n",
    "    \"weight_decay\": [0.0, 0.01, 0.1],\n",
    "    \"batch_size\": [32, 64, 128, 256, 512, 1024],\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "OPTIMIZER_NAMES = ['AdamW', 'MuonW', \"ManifoldMuonW\"]\n",
    "\n",
    "OPTIMIZER_GRID_REGISTRY = {\n",
    "    \"AdamW\": HYPERPARAM_GRID_ADAMW,\n",
    "    \"MuonW\": HYPERPARAM_GRID_MUON,\n",
    "    \"ManifoldMuonW\": HYPERPARAM_GRID_MUON,\n",
    "}\n",
    "\n",
    "MODEL_ARCHS = [\"rms\", \"standard\", \"none\"]\n",
    "\n",
    "def short_hparam_str(hparams: dict, max_len: int = 128) -> str:\n",
    "    \"\"\"\n",
    "    Turn a small hyperparam dict into a compact, filesystem-safe string.\n",
    "    Example: {'lr':1e-3,'wd':0.1} -> 'lr1e-3_wd0.1' (possibly truncated + hash).\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for k, v in hparams.items():\n",
    "        # Normalize floats for readability\n",
    "        if isinstance(v, float):\n",
    "            v_str = f\"{v:.1e}\" if (v < 0.01 or v > 1000) else str(v)\n",
    "        else:\n",
    "            v_str = str(v)\n",
    "        parts.append(f\"{k}{v_str}\")\n",
    "    base = \"_\".join(parts)\n",
    "    if len(base) <= max_len:\n",
    "        return base\n",
    "    # Truncate and append hash so we keep uniqueness but stay short\n",
    "    h = hashlib.md5(base.encode(\"utf-8\")).hexdigest()[:6]\n",
    "    return base[: max_len - 7] + \"_\" + h\n",
    "\n",
    "\n",
    "def iter_hparam_configs(hyperparam_grid: dict):\n",
    "    \"\"\"\n",
    "    Given {\"lr\":[1e-4,1e-3], \"wd\":[0.0,0.1]}, yield:\n",
    "        {\"lr\":1e-4,\"wd\":0.0}, {\"lr\":1e-4,\"wd\":0.1}, ...\n",
    "    \"\"\"\n",
    "    keys = list(hyperparam_grid.keys())\n",
    "    values = [hyperparam_grid[k] for k in keys]\n",
    "    for combo in product(*values):\n",
    "        yield dict(zip(keys, combo))\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Generate hyperparameter sweep configuration files.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--xy_size\",\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help=\"Input feature dimensionality (D).\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_pairs\",\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help=\"Number of (x, y) pairs per batch (T).\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--project_name\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"WandB project name for all generated configs.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--last_k\",\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help=\"Number of recent losses to average for run summary.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"jobs\",\n",
    "        help=\"Directory in which to save all generated config files.\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    xy_size = args.xy_size\n",
    "    num_pairs = args.num_pairs\n",
    "    project_name = args.project_name\n",
    "    last_k = args.last_k\n",
    "    root = args.output_dir\n",
    "\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    print(\"\\n=== Generating sweep configs ===\")\n",
    "    for optimizer_name in OPTIMIZER_NAMES:\n",
    "        opt_grid = OPTIMIZER_GRID_REGISTRY[optimizer_name]\n",
    "        opt_dir = os.path.join(root, optimizer_name)\n",
    "        os.makedirs(opt_dir, exist_ok=True)\n",
    "\n",
    "        for arch_name in MODEL_ARCHS:\n",
    "            arch_dir = os.path.join(opt_dir, arch_name)\n",
    "            os.makedirs(arch_dir, exist_ok=True)\n",
    "            print(f\"\\nOptimizer: {optimizer_name}, Arch: {arch_name}\")\n",
    "            hparam_dicts = list(iter_hparam_configs(opt_grid))\n",
    "            for idx, hparams in enumerate(hparam_dicts):\n",
    "                batch_size = hparams[\"batch_size\"]\n",
    "                optimizer_kwargs = {k: v for k, v in hparams.items() if k != \"batch_size\"}\n",
    "                hparam_str = short_hparam_str(hparams)\n",
    "                run_name = f\"{optimizer_name}_{arch_name}_{hparam_str}\"\n",
    "                spec = {\n",
    "                    \"run_name\": run_name,\n",
    "                    \"arch_name\": arch_name,\n",
    "                    \"optimizer_name\": optimizer_name,\n",
    "                    \"optimizer_kwargs\": optimizer_kwargs,\n",
    "                    \"xy_size\": xy_size,\n",
    "                    \"num_pairs\": num_pairs,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"project_name\": project_name,\n",
    "                    \"last_k\": last_k,\n",
    "                }\n",
    "                # job_000.json naming\n",
    "                job_id = f\"{idx:03d}\"\n",
    "                out_path = os.path.join(arch_dir, f\"job_{job_id}.json\")\n",
    "                with open(out_path, \"w\") as f:\n",
    "                    json.dump(spec, f, indent=2)\n",
    "                #print(f\"  wrote {out_path}\")\n",
    "    print(\"\\n=== Sweep generation complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3cbefb",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568bb4dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import os\n",
    "import glob\n",
    "from collections import deque\n",
    "import time\n",
    "import wandb\n",
    "from optimizers.muonW1 import MuonW\n",
    "from optimizers.manifold_muonW import ManifoldMuonW\n",
    "from loadtypes.config_types import OptimizerKwargs, ExperimentConfig\n",
    "from model.model import make_model\n",
    "from scripts.dataset import get_batch as get_ols_batch\n",
    "import datetime\n",
    "\n",
    "# Optional TPU support\n",
    "try:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    HAS_XLA = True\n",
    "except ImportError:\n",
    "    HAS_XLA = False\n",
    "\n",
    "def resolve_device_and_saver(device_str: str):\n",
    "    \"\"\"\n",
    "    Returns (torch.device-like, save_fn, optimizer_step_fn).\n",
    "    \"\"\"\n",
    "    if device_str.lower() == \"tpu\":\n",
    "        if not HAS_XLA:\n",
    "            raise RuntimeError(\"TPU requested but torch_xla is not installed.\")\n",
    "        device = xm.xla_device()\n",
    "        save_fn = xm.save\n",
    "\n",
    "        def optimizer_step_fn(optimizer):\n",
    "            xm.optimizer_step(optimizer)\n",
    "            xm.mark_step()\n",
    "\n",
    "    else:\n",
    "        device = torch.device(device_str)\n",
    "        save_fn = torch.save\n",
    "\n",
    "        def optimizer_step_fn(optimizer):\n",
    "            optimizer.step()\n",
    "\n",
    "    return device, save_fn, optimizer_step_fn\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, step: int, path: str, scheduler=None, save_fn=torch.save):\n",
    "    ckpt = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict() if scheduler else None,\n",
    "        \"step\": step,\n",
    "        \"rng_state\": torch.random.get_rng_state(),\n",
    "        \"cuda_rng_state\": torch.cuda.get_rng_state() if torch.cuda.is_available() else None,\n",
    "    }\n",
    "    save_fn(ckpt, path)\n",
    "    print(f\"[checkpoint] saved to {path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, path: str, device=\"cuda\", scheduler=None) -> int:\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    print(\"Loaded checkpoint keys:\", ckpt.keys())\n",
    "\n",
    "    # Handle old vs new key names\n",
    "    model_key = \"model\" if \"model\" in ckpt else \"model_state\"\n",
    "    optim_key = \"optimizer\" if \"optimizer\" in ckpt else \"optimizer_state\"\n",
    "\n",
    "    model.load_state_dict(ckpt[model_key])\n",
    "    optimizer.load_state_dict(ckpt[optim_key])\n",
    "\n",
    "    if scheduler is not None and \"scheduler\" in ckpt and ckpt[\"scheduler\"] is not None:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "\n",
    "    # RNG state is optional – only restore if present\n",
    "    \"\"\" if \"rng_state\" in ckpt:\n",
    "        torch.random.set_rng_state(ckpt[\"rng_state\"])\n",
    "    if \"cuda_rng_state\" in ckpt and ckpt[\"cuda_rng_state\"] is not None and torch.cuda.is_available():\n",
    "        torch.cuda.set_rng_state(ckpt[\"cuda_rng_state\"]) \"\"\"\n",
    "\n",
    "    print(f\"[checkpoint] resumed from {path}\")\n",
    "    return ckpt.get(\"step\", 0)\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir: str) -> str | None:\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        return None\n",
    "\n",
    "    files = glob.glob(os.path.join(checkpoint_dir, \"step_*.pt\"))\n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    best_path = None\n",
    "    best_step = -1\n",
    "\n",
    "    for path in files:\n",
    "        base = os.path.basename(path)  # e.g. \"step_2000_20251119-154210.pt\"\n",
    "        if not base.startswith(\"step_\"):\n",
    "            continue\n",
    "        parts = base.split(\"_\")\n",
    "        # [\"step\", \"<step>\", \"<timestamp>.pt\"]\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "\n",
    "        step_str = parts[1]\n",
    "        try:\n",
    "            step = int(step_str)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        if step > best_step:\n",
    "            best_step = step\n",
    "            best_path = path\n",
    "\n",
    "    return best_path\n",
    "\n",
    "\n",
    "class WarmupConstantDecayLrScheduler:\n",
    "    def __init__(self, optimizer, total_steps, warmup_ratio=0.02, decay_ratio=0.10):\n",
    "        self.optimizer = optimizer\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = int(total_steps * warmup_ratio)\n",
    "        self.decay_steps = int(total_steps * decay_ratio)\n",
    "        self.decay_start = total_steps - self.decay_steps\n",
    "        self.base_lrs = [g['lr'] for g in optimizer.param_groups]\n",
    "        self.last_step = 0\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {\"last_step\": self.last_step}\n",
    "\n",
    "    def load_state_dict(self, state):\n",
    "        self.last_step = state[\"last_step\"]\n",
    "\n",
    "    def step(self):\n",
    "        step = self.last_step\n",
    "        if step < self.warmup_steps and self.warmup_steps != 0:\n",
    "            scale = step / self.warmup_steps\n",
    "        elif step < self.decay_start:\n",
    "            scale = 1.0\n",
    "        else:\n",
    "            remaining = max(1, self.total_steps - self.decay_start)\n",
    "            scale = max(0.0, 1 - (step - self.decay_start) / remaining)\n",
    "\n",
    "        for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups):\n",
    "            group[\"lr\"] = base_lr * scale\n",
    "        self.last_step += 1\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    logger,\n",
    "    *,\n",
    "    get_batch,\n",
    "    batch_size=8,\n",
    "    num_pairs=5,\n",
    "    xy_size=5,\n",
    "    num_steps=1000,\n",
    "    device=\"cuda\",\n",
    "    verbose=False,\n",
    "    print_interval=1000,\n",
    "    checkpoint_every=20,\n",
    "    checkpoint_dir=None,\n",
    "    resume_from: str | None = None,\n",
    "    scheduler=None,\n",
    "):\n",
    "    device, save_fn, optimizer_step_fn = resolve_device_and_saver(device)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    if checkpoint_dir is not None:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    prev_step = 0\n",
    "    if resume_from:\n",
    "        prev_step = load_checkpoint(model, optimizer, resume_from, device=device, scheduler=scheduler)\n",
    "       \n",
    "    for step in range(prev_step, num_steps):\n",
    "        iter_start = time.time()\n",
    "        tokens, X, Y, W, x_token_indices = get_batch(\n",
    "            batch_size=batch_size,\n",
    "            num_pairs=num_pairs,\n",
    "            xy_size=xy_size,\n",
    "            device=device,\n",
    "        )\n",
    "        outputs = model(tokens)\n",
    "        B, S, D = outputs.shape\n",
    "        b_idx = torch.arange(B, device=device).unsqueeze(1)\n",
    "        y_pred = outputs[b_idx, x_token_indices, :]\n",
    "        loss = torch.sum((y_pred-Y)**2, dim=1).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_step_fn(optimizer)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if checkpoint_dir is not None and step % checkpoint_every == 0 and step != 0:\n",
    "            now = datetime.datetime.now()\n",
    "            timestamp = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            ckpt_path = os.path.join(checkpoint_dir, f\"step_{step+1}_{timestamp}.pt\")\n",
    "            save_checkpoint(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                step=step + 1,\n",
    "                path=ckpt_path,\n",
    "                scheduler=scheduler,\n",
    "                save_fn=save_fn,   # torch.save or xm.save\n",
    "            )\n",
    "\n",
    "            #add wandb artifact logging\n",
    "            if logger is not None and hasattr(logger, \"run\"):\n",
    "                artifact = wandb.Artifact(\n",
    "                    name=f\"ckpt_step_{step+1}\",\n",
    "                    type=\"model\",\n",
    "                    metadata={\n",
    "                        \"step\": step + 1,\n",
    "                        \"arch\": getattr(model, \"__class__\", type(model)).__name__,\n",
    "                    },\n",
    "                )\n",
    "                artifact.add_file(ckpt_path)\n",
    "                logger.run.log_artifact(artifact)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"[Step {step}] Saved checkpoint to {ckpt_path}\")\n",
    "\n",
    "        if verbose and (step % print_interval == 0):\n",
    "            print(f\"[Step {step}] loss = {loss.item():.6f}\")\n",
    "\n",
    "        if logger is not None:\n",
    "            iter_sec = time.time() - iter_start\n",
    "            logger.log({\"train/loss\": loss.item(), \"step\": step, \"train/iter_sec\": iter_sec})\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class WandbLossLogger:\n",
    "    \"\"\"\n",
    "    Wraps a wandb.Run-like object to:\n",
    "      - forward logs to wandb\n",
    "      - keep a rolling window of the last K 'loss' values\n",
    "    \"\"\"\n",
    "    def __init__(self, run, last_k: int = 50):\n",
    "        self.start_time = time.time()\n",
    "        self.run = run\n",
    "        self.last_k = deque(maxlen=last_k)\n",
    "    \n",
    "    def log(self, metrics: dict):\n",
    "        if \"train/loss\" in metrics:\n",
    "            self.last_k.append(metrics[\"train/loss\"])\n",
    "        self.run.log(metrics)\n",
    "    \n",
    "    def get_last_k_loss(self):\n",
    "        return sum(self.last_k) / len(self.last_k)\n",
    "    \n",
    "    def finish(self):\n",
    "        self.run.finish()\n",
    "\n",
    "\n",
    "OPTIMIZER_REGISTRY = {\n",
    "    \"AdamW\": torch.optim.AdamW,\n",
    "    \"MuonW\": MuonW,\n",
    "    \"ManifoldMuonW\": ManifoldMuonW,\n",
    "}\n",
    "\n",
    "\n",
    "def run_from_config(config: ExperimentConfig):\n",
    "    \"\"\"\n",
    "    Run a job from a given config.\n",
    "    Returns a dict with summary stats.\n",
    "    \"\"\"\n",
    "    experiment_phase: str = config[\"experiment_phase\"]\n",
    "    run_name: str = config[\"run_name\"]\n",
    "    arch_name: str = config[\"arch_name\"]\n",
    "    optimizer_name: str = config[\"optimizer_name\"]\n",
    "    optimizer_kwargs: OptimizerKwargs = config[\"optimizer_kwargs\"]\n",
    "    xy_size: int = config[\"xy_size\"]\n",
    "    num_pairs: int = config[\"num_pairs\"]\n",
    "    num_steps: int = config[\"num_steps\"]\n",
    "    batch_size: int = config[\"batch_size\"]\n",
    "    checkpoint_every: int = config[\"checkpoint_every\"]\n",
    "    device: str = config[\"device\"]\n",
    "    project_name: str = config[\"project_name\"]\n",
    "    base_ckpt_dir: str = config[\"base_ckpt_dir\"]\n",
    "    last_k: int = config[\"last_k\"]\n",
    "\n",
    "    ckpt_dir = os.path.join(\n",
    "        base_ckpt_dir,\n",
    "        experiment_phase,\n",
    "        optimizer_name,\n",
    "        arch_name,\n",
    "        run_name,\n",
    "    )\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    resume_from = find_latest_checkpoint(ckpt_dir)\n",
    "\n",
    "    group_name = f\"{experiment_phase}/{optimizer_name}/{arch_name}\"\n",
    "\n",
    "    run = wandb.init(\n",
    "        id=run_name,\n",
    "        project=project_name,\n",
    "        name=run_name,  # wandb name limit\n",
    "        group=group_name,\n",
    "        config=config,\n",
    "        reinit=True,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "\n",
    "    logger = WandbLossLogger(run, last_k=last_k)\n",
    "    model = make_model(arch_name)\n",
    "    optimizer_class = OPTIMIZER_REGISTRY[optimizer_name]\n",
    "\n",
    "    opt_kwargs = {}\n",
    "    \n",
    "    if \"lr\" in optimizer_kwargs:\n",
    "        opt_kwargs[\"lr\"] = optimizer_kwargs[\"lr\"]\n",
    "    if \"weight_decay\" in optimizer_kwargs:\n",
    "        opt_kwargs[\"weight_decay\"] = optimizer_kwargs[\"weight_decay\"]\n",
    "\n",
    "    # Handle beta1 / beta2 -> betas, if they exist\n",
    "    if \"beta1\" in optimizer_kwargs and \"beta2\" in optimizer_kwargs:\n",
    "        opt_kwargs[\"betas\"] = (optimizer_kwargs[\"beta1\"], optimizer_kwargs[\"beta2\"])\n",
    "\n",
    "    # Muon / Manifold Muon might have other fields, e.g. \"momentum\"\n",
    "    # Pass any remaining optimizer-specific keys explicitly if you need:\n",
    "    for k in [\"momentum\", \"nesterov\"]:\n",
    "        if k in optimizer_kwargs:\n",
    "            opt_kwargs[k] = optimizer_kwargs[k]\n",
    "\n",
    "    optimizer = optimizer_class(model.parameters(), **opt_kwargs)\n",
    "\n",
    "    scheduler = WarmupConstantDecayLrScheduler(optimizer, num_steps)\n",
    "    model = train(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        logger=logger,\n",
    "        get_batch=get_ols_batch,\n",
    "        batch_size=batch_size,\n",
    "        num_pairs=num_pairs,\n",
    "        xy_size=xy_size,\n",
    "        num_steps=num_steps,\n",
    "        device=device,\n",
    "        checkpoint_every=checkpoint_every,\n",
    "        checkpoint_dir=ckpt_dir,\n",
    "        resume_from=resume_from,\n",
    "        verbose=True,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "\n",
    "    avg_last_k_loss = logger.get_last_k_loss()\n",
    "    logger.log({\"avg_last_k_train_loss\": avg_last_k_loss})\n",
    "    logger.finish()\n",
    "\n",
    "    return {\"avg_last_k_train_loss\": avg_last_k_loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7818a",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ea2ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "  def __init__(self, head_dim, base=10000):\n",
    "    super().__init__()\n",
    "    self.head_dim = head_dim\n",
    "    self.base = base\n",
    "  \n",
    "  def calc_inv_freqs(self):\n",
    "    inv_freqs = -2 * torch.arange(self.head_dim // 2) / self.head_dim\n",
    "    inv_freqs = self.base ** inv_freqs\n",
    "    return inv_freqs\n",
    "  \n",
    "  def calc_cos_sin(self, num_tokens):\n",
    "    inv_freqs = self.calc_inv_freqs()\n",
    "    t = torch.arange(num_tokens)\n",
    "    freqs = torch.einsum(\"i,j->ij\", t, inv_freqs)\n",
    "    cos = freqs.cos()\n",
    "    sin = freqs.sin()\n",
    "    return cos, sin\n",
    "  \n",
    "  def apply_rotary_emb(self, x, cos, sin):\n",
    "    # t, d/2 = cos.shape\n",
    "    # t, d/2 = sin.shape\n",
    "    # b, h, t, d = x.shape\n",
    "    x1, x2 = torch.chunk(x, 2, dim=-1)\n",
    "    # b, h, t, d/2 = x1.shape\n",
    "    # b, h, t, d/2 = x2.shape\n",
    "    o1 = x1 * cos - x2 * sin\n",
    "    o2 = x1 * sin + x2 * cos\n",
    "    # absolute position of rotated features doesn't matter as long as it's consistent in q and k in dot prod\n",
    "    return torch.cat([o1, o2], dim=-1)\n",
    "\n",
    "  def forward(self, q, k):\n",
    "    num_tokens = q.shape[2]\n",
    "    cos, sin = self.calc_cos_sin(num_tokens)\n",
    "    cos, sin = cos.to(q.device), sin.to(q.device)\n",
    "    q = self.apply_rotary_emb(q, cos, sin)\n",
    "    k = self.apply_rotary_emb(k, cos, sin)\n",
    "    return q, k\n",
    "    \n",
    "class RMSNorm(nn.Module):\n",
    "  def __init__(self, num_features, eps=1e-5, learnable=True):\n",
    "    super().__init__()\n",
    "    self.num_features = num_features\n",
    "    self.eps = eps\n",
    "    self.learnable = learnable\n",
    "    if self.learnable:\n",
    "      self.scale = nn.Parameter(torch.ones(num_features))\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x_norm = x * torch.rsqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)\n",
    "    if self.learnable:\n",
    "      return x_norm * self.scale\n",
    "    return x_norm\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, num_features, learnable=True, eps=1e-5):\n",
    "    super().__init__()\n",
    "    self.num_features = num_features\n",
    "    self.eps = eps\n",
    "    self.learnable = learnable\n",
    "    self.scale = nn.Parameter(torch.ones(num_features))\n",
    "    self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    x_norm = (x - mean) * torch.rsqrt(variance + self.eps)\n",
    "    return x_norm * self.scale #+ self.bias\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model = 256, n_heads = 8):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.n_heads = n_heads\n",
    "    self.d_k = d_model // n_heads\n",
    "    self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "    self.out = nn.Linear(d_model, d_model, bias=False)\n",
    "    self.rope = RotaryEmbedding(self.d_k)\n",
    "\n",
    "  def sdpa(self, Q, K, V):\n",
    "    B, H, T, D = Q.shape\n",
    "    Q, K = self.rope(Q, K)\n",
    "    attn_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    attn_scores = attn_scores / self.d_k ** 0.5\n",
    "    mask = torch.tril(torch.ones(T, T, device=Q.device))\n",
    "    attn_scores= attn_scores.masked_fill(mask == 0, -float(\"inf\"))\n",
    "    attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "    out = torch.matmul(attn_probs, V)\n",
    "    return out, attn_probs\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    b, t, d = x.shape\n",
    "    return x.view(b, t, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "  def combine_heads(self, x):\n",
    "    b, _, t, d = x.shape\n",
    "    return x.transpose(1, 2).contiguous().view(b, t, self.d_model)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    b, t, d = x.shape\n",
    "    qkv = self.qkv(x)\n",
    "    q = qkv[:, :, :self.d_model].contiguous()\n",
    "    k = qkv[:, :, self.d_model:2*self.d_model].contiguous()\n",
    "    v = qkv[:, :, 2*self.d_model:].contiguous()\n",
    "    q = self.split_heads(q)\n",
    "    k = self.split_heads(k)\n",
    "    v = self.split_heads(v)\n",
    "    attn_out, attn_probs = self.sdpa(q, k, v)\n",
    "    output = self.out(self.combine_heads(attn_out))\n",
    "    return output, attn_probs\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "  def __init__(self, n_layers=15, hidden_size=256, n_heads=8, norm_fn=None):\n",
    "    super().__init__()\n",
    "    self.n_layers = n_layers\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_heads = n_heads\n",
    "    self.has_norm = norm_fn is not None\n",
    "    if self.has_norm:\n",
    "      self.norm = norm_fn(hidden_size)\n",
    "    self.mha = MultiHeadAttention(hidden_size, n_heads)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    if self.has_norm:\n",
    "      t = self.norm(x)\n",
    "    else:\n",
    "      t = x\n",
    "    t, _ = self.mha(t)\n",
    "    return t / self.n_layers + x * (self.n_layers - 1) / self.n_layers\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "  def __init__(self, hidden_size=256):\n",
    "      super().__init__()\n",
    "      self.fc1 = nn.Linear(hidden_size, 2 * 2 * hidden_size, bias=False)\n",
    "      self.fc2 = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "      self.beta = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "  def forward(self, x):\n",
    "      x_proj = self.fc1(x)\n",
    "      x_main, x_gate = x_proj.chunk(2, dim=-1)\n",
    "      gate = x_gate * torch.sigmoid(self.beta * x_gate)\n",
    "      x = x_main * gate\n",
    "      return self.fc2(x)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, n_layers=15, hidden_size=256, norm_fn=None, ):\n",
    "    super().__init__()\n",
    "    self.n_layers = n_layers\n",
    "    self.hidden_size = hidden_size\n",
    "    self.has_norm = norm_fn is not None\n",
    "    if self.has_norm:\n",
    "      self.norm = norm_fn(hidden_size)\n",
    "    self.swiglu = SwiGLU(hidden_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.has_norm:\n",
    "      t = self.norm(x)\n",
    "    else:\n",
    "      t = x\n",
    "    t = self.swiglu(t)\n",
    "    return t / self.n_layers + x * (self.n_layers - 1) / self.n_layers\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, n_layers=15, hidden_size=256, n_heads=8, norm_fn=None):\n",
    "    super().__init__()\n",
    "    self.n_layers = n_layers\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_heads = n_heads\n",
    "    self.attn = AttentionBlock(n_layers, hidden_size, n_heads, norm_fn=norm_fn)\n",
    "    self.mlp = MLP(n_layers, hidden_size, norm_fn=norm_fn)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.attn(x)\n",
    "    x = self.mlp(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self,\n",
    "                hidden_size=256, \n",
    "                n_heads=8, \n",
    "                n_layers=15, \n",
    "                xy_size=5, \n",
    "                norm_fn=lambda d: RMSNorm(d, learnable=False)):\n",
    "    super().__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_heads = n_heads\n",
    "    self.n_layers = n_layers\n",
    "    self.xy_size = xy_size\n",
    "    self.blocks = nn.ModuleList([TransformerBlock(n_layers, hidden_size, n_heads, norm_fn=norm_fn) for _ in range(n_layers)])\n",
    "    self.embedding = nn.Linear(2 * (xy_size + 1), hidden_size, bias=False)\n",
    "    # emb should NOT use standard Xavier initialization\n",
    "    # we can calculate and see that we need to scale by (xy_size + 1)**-0.5 to get the activation rms norm to be 1\n",
    "    nn.init.normal_(self.embedding.weight, mean=0.0, std=(xy_size + 1)**-0.5)\n",
    "    self.has_norm = norm_fn is not None\n",
    "    if self.has_norm:\n",
    "      self.norm = norm_fn(hidden_size)\n",
    "    self.unembedding = nn.Linear(hidden_size, xy_size, bias=False)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    for block in self.blocks:\n",
    "      x = block(x)\n",
    "    if self.has_norm:\n",
    "      x = self.norm(x)\n",
    "    x = self.unembedding(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_model(arch_name):\n",
    "    if arch_name == \"rms\":\n",
    "      ln = lambda d: RMSNorm(d, learnable=False)\n",
    "    elif arch_name == \"standard\":\n",
    "      ln = lambda d: LayerNorm(d, learnable=True)\n",
    "    else:\n",
    "      ln = None\n",
    "    transformer = Transformer(hidden_size=256, n_heads=8, n_layers=15, xy_size=5, norm_fn=ln)\n",
    "    return transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d369a",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019b845",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "from typing import cast\n",
    "from scripts.training import run_from_config\n",
    "from loadtypes.config_types import ExperimentSpec, RunOptions, ExperimentConfig\n",
    "\n",
    "def load_spec(path: str) -> ExperimentSpec:\n",
    "    \"\"\"Load only the experiment specification (static part) from JSON.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return cast(ExperimentSpec, data)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Run training from a config file.\")\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to JSON experiment spec file.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--phase\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Experiment phase (e.g. sweep, exp1, ablation1).\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-steps\",\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help=\"Number of steps to train on.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Device to run on: cpu, cuda, tpu, auto.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ckpt-root\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Base checkpoint directory.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-id\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Id of the current job.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint_every\",\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help=\"how often to checkpoint.\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(f\"\\n=== Loading experiment spec from {args.config} ===\")\n",
    "    spec = load_spec(args.config)\n",
    "    run_options: RunOptions = {\n",
    "        \"experiment_phase\": args.phase,\n",
    "        \"device\": args.device,\n",
    "        \"base_ckpt_dir\": args.ckpt_root,\n",
    "        \"num_steps\": args.num_steps,\n",
    "        \"checkpoint_every\": args.checkpoint_every,\n",
    "    }\n",
    "\n",
    "    config: ExperimentConfig = {**spec, **run_options}\n",
    "    print(\"\\n=== Starting training run ===\")\n",
    "    result = run_from_config(config)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING RUN COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Run name:                 {spec['run_name']}\")\n",
    "    print(f\"Experiment phase:         {run_options['experiment_phase']}\")\n",
    "    print(f\"Optimizer:                {spec['optimizer_name']}\")\n",
    "    print(f\"Architecture:             {spec['arch_name']}\")\n",
    "    print(f\"Avg last-k train loss:    {result['avg_last_k_train_loss']:.6f}\")\n",
    "    print(f\"Checkpoint directory:     {run_options['base_ckpt_dir']}\")\n",
    "    print(f\"Checkpoint every:     {run_options['checkpoint_every']}\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433448a5",
   "metadata": {},
   "source": [
    "## run job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97b284",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "set -e\n",
    "\n",
    "if [ $# -ne 8 ]; then\n",
    "    echo \"Usage: $0 OPTIMIZER ARCH JOB_ID PHASE DEVICE CKPT_ROOT NUM_STEPS CHECKPOINT_EVERY\"\n",
    "    echo \"Example: $0 muon rms 003 sweep cuda checkpoints 3000 200\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "OPTIMIZER=$1\n",
    "ARCH=$2\n",
    "JOB_ID=$3\n",
    "PHASE=$4\n",
    "DEVICE=$5\n",
    "CKPT_ROOT=$6\n",
    "NUM_STEPS=$7\n",
    "CHECKPOINT_EVERY=$8\n",
    "\n",
    "CONFIG=\"jobs/${OPTIMIZER}/${ARCH}/job_${JOB_ID}.json\"\n",
    "\n",
    "echo \"Running config: $CONFIG\"\n",
    "python3 main.py \\\n",
    "    --config \"$CONFIG\" \\\n",
    "    --phase \"$PHASE\" \\\n",
    "    --num-steps \"$NUM_STEPS\" \\\n",
    "    --device \"$DEVICE\" \\\n",
    "    --ckpt-root \"$CKPT_ROOT\" \\\n",
    "    --checkpoint_every \"$CHECKPOINT_EVERY\" \\\n",
    "    --job-id \"$JOB_ID\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e788e5",
   "metadata": {},
   "source": [
    "## run jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0251d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "set -e\n",
    "\n",
    "# Usage:\n",
    "#   ./run_jobs_local.sh OPTIMIZER ARCH START_ID END_ID PHASE DEVICE CKPT_ROOT NUM_STEPS CHECKPOINT_EVERY [NUM_GPUS]\n",
    "# Example:\n",
    "#   ./run_jobs_local.sh AdamW rms 0 15 sweep cuda checkpoints 3000 200 4\n",
    "#\n",
    "# If NUM_GPUS is omitted, defaults to 1 (serial).\n",
    "\n",
    "if [ \"$#\" -lt 9 ] || [ \"$#\" -gt 10 ]; then\n",
    "  echo \"Usage: $0 OPTIMIZER ARCH START_ID END_ID PHASE DEVICE CKPT_ROOT NUM_STEPS CHECKPOINT_EVERY [NUM_GPUS]\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "OPTIMIZER=$1    # e.g. AdamW\n",
    "ARCH=$2         # e.g. rms\n",
    "START=$3        # e.g. 0\n",
    "END=$4          # e.g. 15\n",
    "PHASE=$5        # e.g. sweep\n",
    "DEVICE=$6       # cpu | cuda | tpu | auto\n",
    "CKPT_ROOT=$7    # e.g. checkpoints\n",
    "NUM_STEPS=$8    # e.g. 3000\n",
    "CHECKPOINT_EVERY=$9\n",
    "NUM_GPUS=${10:-1}   # Optional; default 1\n",
    "\n",
    "echo \"Using up to ${NUM_GPUS} GPU(s)\"\n",
    "\n",
    "# simple concurrency limiter\n",
    "active_jobs=0\n",
    "\n",
    "for i in $(seq \"$START\" \"$END\"); do\n",
    "  JOB_ID=$(printf \"%03d\" \"$i\")\n",
    "  GPU_ID=$(( i % NUM_GPUS ))   # round-robin assignment\n",
    "\n",
    "  echo \"=== Launching job ${JOB_ID} on GPU ${GPU_ID} (${OPTIMIZER}, ${ARCH}) ===\"\n",
    "\n",
    "  if [ \"$DEVICE\" = \"cuda\" ] || [ \"$DEVICE\" = \"auto\" ]; then\n",
    "    # Bind this job to one GPU\n",
    "    CUDA_VISIBLE_DEVICES=$GPU_ID \\\n",
    "      scripts/run_job.sh \"$OPTIMIZER\" \"$ARCH\" \"$JOB_ID\" \"$PHASE\" \"cuda\" \"$CKPT_ROOT\" \"$NUM_STEPS\" \"$CHECKPOINT_EVERY\" &\n",
    "  else\n",
    "    # CPU / TPU case: no CUDA_VISIBLE_DEVICES\n",
    "    scripts/run_job.sh \"$OPTIMIZER\" \"$ARCH\" \"$JOB_ID\" \"$PHASE\" \"$DEVICE\" \"$CKPT_ROOT\" \"$NUM_STEPS\" \"$CHECKPOINT_EVERY\" &\n",
    "  fi\n",
    "\n",
    "  active_jobs=$((active_jobs + 1))\n",
    "\n",
    "  # if we already have NUM_GPUS jobs running, wait for one to finish\n",
    "  if [ \"$active_jobs\" -ge \"$NUM_GPUS\" ]; then\n",
    "    # wait for any one background job to finish (bash 4.3+)\n",
    "    wait -n\n",
    "    active_jobs=$((active_jobs - 1))\n",
    "  fi\n",
    "done\n",
    "\n",
    "# wait for all remaining jobs\n",
    "wait\n",
    "echo \"All jobs finished.\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
